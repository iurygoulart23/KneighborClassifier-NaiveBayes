{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Where tweets from with Naive Bayes Machine Learning\n",
    "\n",
    "i will use a Naive Bayes Classifier to find patterns in real tweets. i have three files: `new_york.json`, `london.json`, and `paris.json`. These three files contain tweets that i gathered from those locations.\n",
    "\n",
    "- The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris.\n",
    "\n",
    "### Investigate the Data\n",
    "\n",
    "To begin, let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "Be best #ThursdayThoughts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "print(len(new_york_tweets))\n",
    "print(new_york_tweets.columns)\n",
    "print(new_york_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the block below the London and Paris tweets into DataFrames named `london_tweets` and `paris_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5341 London tweets\n",
      "There are 2510 Paris tweets\n"
     ]
    }
   ],
   "source": [
    "london_tweets = pd.read_json('london.json', lines=True)\n",
    "paris_tweets = pd.read_json('paris.json', lines=True)\n",
    "\n",
    "print('There are {} London tweets'.format(len(london_tweets)))\n",
    "print('There are {} Paris tweets'.format(len(paris_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying using language: Naive Bayes Classifier\n",
    "\n",
    "We're going to create a Naive Bayes Classifier! Let's begin by looking at the way language is used differently in these three locations. Let's grab the text of all of the tweets and make it one big list. In the code block below, we've created a list of all the New York tweets. Do the same for `london_tweets` and `paris_tweets`.\n",
    "\n",
    "Then combine all three into a variable named `all_tweets` by using the `+` operator. For example, `all_tweets = new_york_text + london_text + ...`\n",
    "\n",
    "Let's also make the labels associated with those tweets. `0` represents a New York tweet, `1`  represents a London tweet, and `2` represents a Paris tweet. Finish the definition of `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the tweets text to a list variable\n",
    "new_york_text = new_york_tweets[\"text\"].tolist()\n",
    "london_text = london_tweets['text'].tolist()\n",
    "paris_text = paris_tweets['text'].tolist()\n",
    "\n",
    "# getting our data and labels to modelling into naive bayers classifier\n",
    "all_tweets = new_york_text + london_text + paris_text\n",
    "labels = [0] * len(new_york_text) + [1] * len(london_text) + [2] * len(paris_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Training and Test Set\n",
    "\n",
    "We can now break our data into a training set and a test set. We'll use scikit-learn's `train_test_split` function to do this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train data is: 9430\n",
      "The length of test data is: 3144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_tweets, labels, random_state=1)\n",
    "\n",
    "print('The length of train data is:', len(train_data))\n",
    "print('The length of test data is:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Count Vectors\n",
    "\n",
    "To use a Naive Bayes Classifier, we need to transform our lists of words into count vectors. Recall that this changes the sentence `\"I love New York, New York\"` into a list that contains:\n",
    "\n",
    "* Two `1`s because the words `\"I\"` and `\"love\"` each appear once.\n",
    "* Two `2`s because the words `\"New\"` and `\"York\"` each appear twice.\n",
    "* Many `0`s because every other word in the training set didn't appear at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Beckenham Hill for the 12.01 today. I had booked assistance &amp; had it confirmed. https://t.co/x1mq7G81rW\n",
      "  (0, 6)\t1\n",
      "  (0, 165)\t1\n",
      "  (0, 2300)\t1\n",
      "  (0, 2883)\t1\n",
      "  (0, 2920)\t1\n",
      "  (0, 3596)\t1\n",
      "  (0, 4238)\t1\n",
      "  (0, 5896)\t1\n",
      "  (0, 6202)\t1\n",
      "  (0, 10145)\t1\n",
      "  (0, 11580)\t2\n",
      "  (0, 12095)\t1\n",
      "  (0, 12417)\t1\n",
      "  (0, 13299)\t1\n",
      "  (0, 25504)\t1\n",
      "  (0, 25947)\t1\n",
      "  (0, 28502)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "\n",
    "counter.fit(train_data)\n",
    "train_counts = counter.transform(train_data)\n",
    "test_counts = counter.transform(test_data)\n",
    "\n",
    "print(train_data[3])\n",
    "print(train_counts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test the Naive Bayes Classifier\n",
    "\n",
    "We now have the inputs to our classifier. Let's use the CountVectors to train and test the Naive Bayes Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(train_counts, train_labels)\n",
    "\n",
    "predictions = classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Your Model\n",
    "\n",
    "Now that the classifier has made its predictions, let's see how well it did. Let's look at two different ways to do this. First, call scikit-learn's `accuracy_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acurracy of the model is: 0.6835241730279898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('The acurracy of the model is:', accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing My Own Tweet\n",
    "\n",
    "The classifier predicts tweets that were actually from New York as either New York tweets or London tweets, but almost never Paris tweets. Similarly, the classifier rarely misclassifies the tweets that were actually from Paris. Tweets coming from two English speaking countries are harder to distinguish than tweets in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = New York, 1 = London, 2 = Paris\n",
      "Your tweet predicion is: [1]\n"
     ]
    }
   ],
   "source": [
    "tweet = ['This is how the brazillians are made: self motivation and strength']\n",
    "\n",
    "tweet_counts = counter.transform(tweet)\n",
    "\n",
    "print('0 = New York, 1 = London, 2 = Paris')\n",
    "print('Your tweet predicion is:', classifier.predict(tweet_counts))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14c5b682b5672cfbdc0b9fb50430e6b8ad6c3d1ee4aebdd911d54f5cf8cc93d8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
