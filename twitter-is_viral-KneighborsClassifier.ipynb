{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter KNeighborsClassifier and NaiveBayes\n",
    "\n",
    "In this project, we are going to use the K-Nearest Neighbor algorithm to predict whether a tweet will go viral. Before jumping into using the classifier, let's first consider the problem we're trying to solve. Which features of a tweet are most closely linked to its popularity?\n",
    "\n",
    "i will use a Naive Bayes Classifier to find patterns in real tweets. i have three files: `new_york.json`, `london.json`, and `paris.json`. These three files contain tweets that i gathered from those locations.\n",
    "\n",
    "- The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris.\n",
    "\n",
    "Let's explore these options by looking at the data we have available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11099\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities',\n",
      "       'metadata', 'source', 'in_reply_to_status_id',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
      "       'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo',\n",
      "       'coordinates', 'place', 'contributors', 'retweeted_status',\n",
      "       'is_quote_status', 'retweet_count', 'favorite_count', 'favorited',\n",
      "       'retweeted', 'lang', 'possibly_sensitive', 'quoted_status_id',\n",
      "       'quoted_status_id_str', 'extended_entities', 'quoted_status',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "RT @KWWLStormTrack7: We are more than a month into summer but the days are getting shorter. The sunrise is about 25 minutes later on July 3â€¦\n",
      "Waterloo, Iowa\n"
     ]
    }
   ],
   "source": [
    "# first lets start with the viral tweets\n",
    "import pandas as pd\n",
    "\n",
    "all_tweets = pd.read_json(\"random_tweets.json\", lines=True)\n",
    "\n",
    "print(len(all_tweets))\n",
    "print(all_tweets.columns)\n",
    "print(all_tweets.loc[0]['text'])\n",
    "\n",
    "#Print the user here and the user's location here.\n",
    "# print(all_tweets[['geo', 'id']].head(20))\n",
    "print(all_tweets['user'].loc[0]['location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Viral Tweets\n",
    "\n",
    "A K-Nearest Neighbor classifier is a supervised machine learning algorithm, and as a result, we need to have a dataset with tagged labels. For this specific example, we need a dataset where every tweet is marked as viral or not viral. Unfortunately, this isn't a feature of our dataset &mdash; we'll need to make it ourselves.\n",
    "\n",
    "So how do we define a viral tweet? A good place to start is to look at the number of retweets the tweet has. This can be found using the feature `\"retweet_count\"`. Let's say we wanted to create a column called `is_viral` that is a `1` if the tweet had more than `5` retweets and `0` otherwise. We could do that like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet median is: 13.0 \n",
      "\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "11094    1\n",
      "11095    1\n",
      "11096    0\n",
      "11097    0\n",
      "11098    0\n",
      "Name: is_viral, Length: 11099, dtype: int32 \n",
      "\n",
      "0    5562\n",
      "1    5537\n",
      "Name: is_viral, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rt_median = np.median(all_tweets['retweet_count'])\n",
    "print('Retweet median is:', rt_median, '\\n')\n",
    "\n",
    "all_tweets['is_viral'] = np.where(all_tweets['retweet_count'] > rt_median, 1, 0)\n",
    "print(all_tweets.is_viral, '\\n')\n",
    "\n",
    "#printing the number of viral tweets\n",
    "print(all_tweets['is_viral'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Features\n",
    "\n",
    "Now that we've created a label for every tweet in our dataset, we can begin thinking about which features might determine whether a tweet is viral. We can create new columns in our dataset to represent these features. For example, let's say we think the length of a tweet might be a valuable feature. The following line creates a new column containing the length of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']), axis=1)\n",
    "all_tweets['followers_count'] = all_tweets.apply(lambda tweet: tweet['user']['followers_count'], axis=1)\n",
    "all_tweets['hashtags_count'] = all_tweets.apply(lambda tweet: tweet['text'].count('#'), axis=1)\n",
    "all_tweets['links_count'] = all_tweets.apply(lambda tweet: tweet['text'].count('http'), axis=1)\n",
    "all_tweets['words_count_in_tweet'] = all_tweets.apply(lambda tweet: len(tweet['text'].split()), axis=1)\n",
    "all_tweets['average_words_length'] = all_tweets.apply(lambda tweet: np.mean(len(tweet['text'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing The Data\n",
    "\n",
    "We've now made the columns that we want to feed into our classifier. Let's get rid of all the data that is no longer relevant. Creating a variable named `labels` and set it equal to the `'is_viral'` column of all_tweets. Them normalizing the variable data with the function 'scale' from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6164054  -0.02878298 -0.32045057 -0.78415588  1.15105133  0.6164054 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "labels = all_tweets.is_viral\n",
    "data = all_tweets[['tweet_length', 'followers_count', 'hashtags_count', 'links_count', 'words_count_in_tweet', 'average_words_length']]\n",
    "\n",
    "scaled_data = scale(data, axis=0)\n",
    "print(scaled_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training Set and Test Set\n",
    "\n",
    "To evaluate the effectiveness of our classifier, we now split `scaled_data` and `labels` into a training set and test set using scikit-learn's `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(scaled_data, labels, test_size=0.2, random_state=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14c5b682b5672cfbdc0b9fb50430e6b8ad6c3d1ee4aebdd911d54f5cf8cc93d8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
